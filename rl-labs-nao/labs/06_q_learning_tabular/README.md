# Lab 06 — Q-Learning

    ## Learning Objectives
    - Implement off-policy TD control
- Stabilise learning with decaying ε
- Explain policy extraction from Q tables

    ## Reading List
    - Watkins & Dayan (1992)
- Sutton & Barto (2018) Chapter 6
- CliffWalking environment documentation

    ## Tasks
    1. Execute the notebook `06_q_learning_cliffwalking.ipynb` start-to-finish using the provided kernel.
    2. Complete the inline questions and implement code where indicated.
    3. Save generated plots to `assets/figures/` (automated cells already handle this).
    4. Submit a short reflection covering insights, challenges, and runtime observations.

    ## Deliverables
    - Exported notebook (`.ipynb` or `.html`) with all cells executed.
    - Figures referenced in the notebook under `assets/figures/`.
    - Summary paragraph answering the lab-specific reflection prompts.

    ## Expected Runtime
    - Approximately 17 minutes on a modern laptop (i7/Ryzen, 16 GB RAM).


## Grading Rubric

- **Correctness (40%)** – unit tests and reference checks pass.
- **Clarity (20%)** – code and narrative are well explained.
- **Analysis (20%)** – figures and discussion interpret results.
- **Reproducibility (20%)** – seeds fixed, runtime within guidance.
