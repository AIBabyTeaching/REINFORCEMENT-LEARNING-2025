{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Problem Statment:\n",
    "Problem Statement:\n",
    "- We'll use reinforcement learning to train an agent to navigate a grid world and reach a goal.\n",
    "- The goal is to reach the bottom-right corner of the grid starting from the top-left corner while minimizing the steps (negative rewards) taken. The agent should learn to navigate the grid efficiently to reach the goal with the highest possible cumulative reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Concepts: \n",
    "1. **Environment**:\\\n",
    "The environment is the world in which the agent operates. It includes everything that the agent interacts with, including the state of the world and the rules governing the state transitions. In our example, the environment is the GridWorld.\n",
    "\n",
    "2. **Grid**:\\\n",
    "The grid is a representation of the environment. It’s a 2D matrix where each cell represents a possible position of the agent. The grid has a start position (top-left corner) and a goal position (bottom-right corner).\n",
    "\n",
    "3. **Agent**:\\\n",
    "The agent is an entity that interacts with the environment by taking actions. The agent's goal is to learn a policy that maximizes cumulative rewards over time. In our example, the agent moves within the grid to reach the goal.\n",
    "\n",
    "4. **State**:\\\n",
    "A state is a specific configuration of the environment. In the grid world, a state is the agent's current position within the grid.\n",
    "\n",
    "5. **Action**:\\\n",
    "An action is a move the agent can make. In our grid world, the actions are moving up, down, left, or right.\n",
    "\n",
    "6. **Reward**:\\\n",
    "A reward is the feedback the agent receives after taking an action. It can be positive or negative, encouraging the agent to take actions that maximize cumulative rewards. In our grid world, the reward is -1 for each step taken and 10 for reaching the goal.\n",
    "\n",
    "7. **Policy**:\\\n",
    "A policy is a strategy used by the agent to determine the next action based on the current state. In Q-learning, the policy is derived from the Q-values.\n",
    "\n",
    "8. **Q-value**:\\\n",
    "The Q-value (quality value) is a measure of the value of a specific action in a specific state, representing the expected cumulative reward of that action.\n",
    "\n",
    "9. **Exploration vs. Exploitation**:\\\n",
    "Exploration is the act of trying new actions to discover their effects, while exploitation is choosing the best-known action to maximize rewards. The ε-greedy strategy balances exploration and exploitation.\n",
    "\n",
    "10. **Episodes**:\\\n",
    "Episodes are sequences of states, actions, and rewards that represent one complete experience of the agent interacting with the environment, starting from an initial state and ending when a taerminal state (or goal) is reached. Each episode is essentially a trial or an attempt for the agent to learn from its actions and the resulting outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Common Parameters:\n",
    "    - Common Parameter Values:\n",
    "    - Learning Rate (alpha): 0.1 (moderate learning speed).\n",
    "    - Discount Factor (gamma): 0.99 (prioritizes future rewards slightly).\n",
    "    - Exploration Rate (epsilon): 1.0 (start with high exploration).\n",
    "    - Epsilon Decay: 0.995 (gradually reduce exploration over episodes).\n",
    "    - Step Penalty: Keep it minimal to encourage exploration, such as -1 per step.\n",
    "    - Goal Reward: Typically a large positive reward, such as 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup Environment\n",
    "We define a `GridWorld` class to represent our environment. The environment consists of a grid where the agent can move up, down, left, or right. The goal is to reach a specific position in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the environment: GridWorld\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    The GridWorld class defines the environment in which the agent operates.\n",
    "    It simulates a grid-based world where the agent can move in four directions:\n",
    "    up, down, left, and right. The goal is for the agent to reach the goal position\n",
    "    while avoiding obstacles.\n",
    "    \n",
    "    Attributes:\n",
    "        grid_size: Tuple representing the dimensions of the grid (rows, columns).\n",
    "        start: Tuple representing the agent's starting position.\n",
    "        goal: Tuple representing the goal position.\n",
    "        obstacles: List of coordinates representing the obstacle positions.\n",
    "        state: The current position of the agent.\n",
    "        actions: A list of possible actions: ['up', 'down', 'left', 'right'].\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size=(5, 5), start=(0, 0), goal=(4, 4), obstacles=None):\n",
    "        \"\"\"\n",
    "        Initializes the GridWorld environment with the specified grid size, start, goal,\n",
    "        and any obstacles.\n",
    "        \n",
    "        Parameters:\n",
    "            grid_size: The size of the grid, a tuple (rows, columns).\n",
    "            start: The starting position of the agent as a tuple (row, column).\n",
    "            goal: The goal position in the grid as a tuple (row, column).\n",
    "            obstacles: A list of coordinates representing obstacles. If no obstacles are\n",
    "                       provided, it defaults to an empty list.\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size  # Size of the grid (rows x columns)\n",
    "        self.start = start  # Agent's starting position\n",
    "        self.goal = goal  # Goal position the agent must reach\n",
    "        self.obstacles = obstacles if obstacles else []  # Obstacle positions, if any\n",
    "        self.state = start  # Initial state (agent starts at the start position)\n",
    "        self.actions = ['up', 'down', 'left', 'right']  # List of possible actions\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to the starting position of the agent.\n",
    "        \n",
    "        Returns:\n",
    "            The initial state (start position) of the agent.\n",
    "        \"\"\"\n",
    "        self.state = self.start  # Reset agent's position to the start\n",
    "        return self.state  # Return the starting state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Moves the agent based on the action provided.\n",
    "        The action must be one of 'up', 'down', 'left', or 'right'.\n",
    "        The method ensures that the agent stays within the grid boundaries and handles\n",
    "        obstacle collisions.\n",
    "        \n",
    "        Parameters:\n",
    "            action: The direction in which the agent will move ('up', 'down', 'left', 'right').\n",
    "        \n",
    "        Returns:\n",
    "            new_state: The new position of the agent after taking the action.\n",
    "            reward: The reward for taking that action. +100 for reaching the goal,\n",
    "                    -1 for each move (small penalty to encourage faster goal-reaching).\n",
    "            done: A boolean indicating whether the agent has reached the goal (True if goal reached).\n",
    "        \"\"\"\n",
    "        row, col = self.state  # Get the current position of the agent\n",
    "        \n",
    "        # Determine the new position based on the action\n",
    "        if action == 'up':\n",
    "            row = max(0, row - 1)  # Move up but prevent moving outside the grid\n",
    "        elif action == 'down':\n",
    "            row = min(self.grid_size[0] - 1, row + 1)  # Move down, staying within bounds\n",
    "        elif action == 'left':\n",
    "            col = max(0, col - 1)  # Move left but stay within bounds\n",
    "        elif action == 'right':\n",
    "            col = min(self.grid_size[1] - 1, col + 1)  # Move right, staying in the grid\n",
    "        \n",
    "        new_state = (row, col)  # Update the agent's position\n",
    "        \n",
    "        # Check if the new position is an obstacle; if so, stay in the same place\n",
    "        if new_state in self.obstacles:\n",
    "            new_state = self.state  # Hit an obstacle, so revert to the previous state\n",
    "\n",
    "        self.state = new_state  # Update the current state to the new state\n",
    "\n",
    "        # Check if the agent reached the goal\n",
    "        if self.state == self.goal:\n",
    "            return new_state, 100, True  # Reward +100 for reaching the goal, episode ends (done=True)\n",
    "        else:\n",
    "            return new_state, -1, False  # Small penalty (-1) for each step, episode continues (done=False)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Visualizes the current state of the environment as a grid.\n",
    "        The agent's position, goal, and obstacles are displayed numerically.\n",
    "        5 represents the agent's current position, 10 represents the goal, and -1 represents obstacles.\n",
    "        \"\"\"\n",
    "        grid = np.zeros(self.grid_size)  # Initialize an empty grid with zeros\n",
    "        grid[self.goal] = 10  # Mark the goal position as 10\n",
    "        \n",
    "        # Mark obstacles as -1\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs] = -1  # Obstacles are represented by -1\n",
    "        \n",
    "        # Mark the agent's current position as 5\n",
    "        grid[self.state] = 5  # Agent's position represented by 5\n",
    "        \n",
    "        # Print the grid to the console for visualization\n",
    "        print(grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Agent\n",
    "We define a `QLearningAgent` class that will learn to navigate the grid using Q-learning.\n",
    "Q-learning is a model-free reinforcement learning algorithm because it does not attempt to build a model of the environment. Instead, it learns an optimal policy directly from the rewards and penalties encountered during interactions with the environment. Model-based methods, on the other hand, involve building and using a model of the environment for planning and decision-making.\n",
    "\n",
    "#### Q-learning Update Rule:\n",
    "$$ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\gamma$ is the discount factor\n",
    "- r is the reward\n",
    "- Q(s, a) is the current state-action value\n",
    "- $\\max_{a'} Q(s', a')$ is the maximum future state-action value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning agent\n",
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    The QLearningAgent class implements the Q-learning algorithm.\n",
    "    It interacts with the GridWorld environment, learning the optimal policy by updating\n",
    "    its Q-values based on rewards received for actions.\n",
    "    \n",
    "    Attributes:\n",
    "        env: The GridWorld environment the agent interacts with.\n",
    "        q_table: A table storing Q-values for state-action pairs. Each state has a Q-value for each possible action.\n",
    "        alpha: Learning rate, controls how much new information overrides old knowledge.\n",
    "        gamma: Discount factor, determines how much future rewards are considered in current decisions.\n",
    "        epsilon: Exploration rate, controls the balance between exploration (random actions) and exploitation (greedy actions).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Q-learning agent with the environment and Q-learning parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            env: The GridWorld environment.\n",
    "            alpha: Learning rate (how much the agent learns from new experiences).\n",
    "            gamma: Discount factor (importance of future rewards).\n",
    "            epsilon: Exploration rate (probability of taking a random action to explore).\n",
    "        \"\"\"\n",
    "        self.env = env  # The GridWorld environment the agent is interacting with\n",
    "        self.q_table = np.zeros((*env.grid_size, len(env.actions)))  # Q-table initialized to zeros\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor (how much we value future rewards)\n",
    "        self.epsilon = epsilon  # Exploration rate (probability of random exploration)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Chooses an action for the agent using an epsilon-greedy strategy.\n",
    "        With probability epsilon, it chooses a random action (exploration),\n",
    "        otherwise it chooses the action with the highest Q-value for the current state (exploitation).\n",
    "        \n",
    "        Parameters:\n",
    "            state: The current position of the agent in the grid.\n",
    "        \n",
    "        Returns:\n",
    "            The action the agent chooses ('up', 'down', 'left', or 'right').\n",
    "        \"\"\"\n",
    "        # Explore with probability epsilon\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.env.actions)  # Random action (explore)\n",
    "        else:\n",
    "            row, col = state  # Get the agent's current position\n",
    "            action_index = np.argmax(self.q_table[row, col])  # Choose the action with the highest Q-value (exploit)\n",
    "            return self.env.actions[action_index]  # Return the chosen action\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Updates the Q-value for the given state-action pair using the Q-learning update rule.\n",
    "        \n",
    "        Parameters:\n",
    "            state: The current state (position) of the agent.\n",
    "            action: The action taken ('up', 'down', 'left', or 'right').\n",
    "            reward: The reward received after taking the action.\n",
    "            next_state: The state the agent ends up in after taking the action.\n",
    "        \"\"\"\n",
    "        row, col = state  # Get the current state (row, col)\n",
    "        next_row, next_col = next_state  # Get the next state (row, col)\n",
    "        action_index = self.env.actions.index(action)  # Get the index of the action\n",
    "\n",
    "        # Q-learning update formula:\n",
    "        # Q(s, a) = Q(s, a) + alpha * [reward + gamma * max(Q(s', a')) - Q(s, a)]\n",
    "        best_next_action = np.max(self.q_table[next_row, next_col])  # Best Q-value for the next state\n",
    "        td_target = reward + self.gamma * best_next_action  # Target value (TD target)\n",
    "        td_error = td_target - self.q_table[row, col, action_index]  # TD error\n",
    "        self.q_table[row, col, action_index] += self.alpha * td_error  # Update the Q-value with learning rate alpha\n",
    "\n",
    "    def train(self, episodes=500):\n",
    "        \"\"\"\n",
    "        Trains the agent by letting it interact with the environment over a specified number of episodes.\n",
    "        In each episode, the agent explores the environment, learns from the rewards, and updates its Q-values.\n",
    "        \n",
    "        Parameters:\n",
    "            episodes: The number of episodes (or iterations) to train the agent for.\n",
    "        \"\"\"\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()  # Reset the environment at the start of each episode\n",
    "            done = False  # The episode continues until the agent reaches the goal\n",
    "            total_reward = 0  # Track the total reward the agent receives in this episode\n",
    "\n",
    "            while not done:\n",
    "                action = self.choose_action(state)  # Choose an action using epsilon-greedy strategy\n",
    "                next_state, reward, done = self.env.step(action)  # Perform the action and observe the result\n",
    "                self.update_q_value(state, action, reward, next_state)  # Update Q-values based on the experience\n",
    "                state = next_state  # Move to the next state\n",
    "                total_reward += reward  # Accumulate the reward\n",
    "\n",
    "            # Not important\n",
    "            # Every 10 episodes, print the total reward for monitoring the learning progress\n",
    "            if episode % 10 == 0:\n",
    "                print(f'Episode {episode}, Total Reward: {total_reward}')\n",
    "\n",
    "    def display_policy_arrows(self):\n",
    "        \"\"\"\n",
    "        Displays the learned policy using arrows to represent the best action from each state.\n",
    "        Arrows are shown on a grid corresponding to the optimal policy for each state.\n",
    "        Obstacles are shown as '#' and the goal as 'G'.\n",
    "        \"\"\"\n",
    "        directions = {0: '↑', 1: '↓', 2: '←', 3: '→'}  # Map actions to arrows for display\n",
    "        fig, ax = plt.subplots(figsize=(7, 7))  # Set up the plot with a 7x7 grid\n",
    "\n",
    "        # Create a grid to visualize obstacles and the goal\n",
    "        grid = np.zeros(self.env.grid_size)  # Initialize a grid with zeros\n",
    "        for obs in self.env.obstacles:\n",
    "            grid[obs] = -1  # Mark obstacles with -1\n",
    "        grid[self.env.goal] = 10  # Mark the goal with 10\n",
    "\n",
    "        # Flip the y-axis so that the origin (0,0) is at the bottom-left for visualization\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        # Display the learned policy as arrows\n",
    "        for row in range(self.env.grid_size[0]):\n",
    "            for col in range(self.env.grid_size[1]):\n",
    "                if (row, col) in self.env.obstacles:\n",
    "                    # Display obstacles as '#'\n",
    "                    ax.text(col, row, '#', va='center', ha='center', fontsize=24, color='black')\n",
    "                elif (row, col) == self.env.goal:\n",
    "                    # Display the goal as 'G'\n",
    "                    ax.text(col, row, 'G', va='center', ha='center', fontsize=24, color='green')\n",
    "                else:\n",
    "                    # Display the best action (as an arrow) for each state\n",
    "                    action_index = np.argmax(self.q_table[row, col])  # Find the best action from the Q-table\n",
    "                    ax.text(col, row, directions[action_index], va='center', ha='center', fontsize=24)\n",
    "\n",
    "        # Format the grid with minor ticks to clearly show each cell boundary\n",
    "        ax.set_xticks(np.arange(-0.5, self.env.grid_size[1], 1), minor=True)\n",
    "        ax.set_yticks(np.arange(-0.5, self.env.grid_size[0], 1), minor=True)\n",
    "        ax.grid(which=\"minor\", color=\"black\", linestyle='-', linewidth=2)  # Draw grid lines\n",
    "        ax.tick_params(which=\"minor\", size=0)  # Remove tick marks\n",
    "        plt.xticks([])  # Hide x-axis ticks\n",
    "        plt.yticks([])  # Hide y-axis ticks\n",
    "        plt.title(\"Learned Policy Arrows\", fontsize=16)  # Add a title to the plot\n",
    "        plt.show()  # Display the plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training the Agent\n",
    "We train the agent by running multiple episodes and updating the Q-table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helper:\n",
    "    @staticmethod\n",
    "    def run_pipeline(grid_size = (5,5), start = (0,0), goal = (4,4), obstacles = [(0,1),(1,1),(1,2),(2,2), (3,3), (4,1)],episodes = 500):\n",
    "        # Create the environment and agent\n",
    "\n",
    "        env = GridWorld(grid_size=grid_size, start=start, goal=goal, obstacles=obstacles)\n",
    "        agent = QLearningAgent(env)\n",
    "\n",
    "        # Train the agent\n",
    "        agent.train(episodes=episodes)\n",
    "\n",
    "        # Display the final learned policy as arrows\n",
    "        agent.display_policy_arrows()\n",
    "\n",
    "        # Test the agent with the learned policy\n",
    "        state = env.reset()\n",
    "        env.render()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            state, _, done = env.step(action)\n",
    "            env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 20\n",
      "Episode 10, Total Reward: 81\n",
      "Episode 20, Total Reward: 86\n",
      "Episode 30, Total Reward: 93\n",
      "Episode 40, Total Reward: 90\n",
      "Episode 50, Total Reward: 93\n",
      "Episode 60, Total Reward: 93\n",
      "Episode 70, Total Reward: 91\n",
      "Episode 80, Total Reward: 93\n",
      "Episode 90, Total Reward: 92\n",
      "Episode 100, Total Reward: 93\n",
      "Episode 110, Total Reward: 93\n",
      "Episode 120, Total Reward: 93\n",
      "Episode 130, Total Reward: 93\n",
      "Episode 140, Total Reward: 93\n",
      "Episode 150, Total Reward: 93\n",
      "Episode 160, Total Reward: 92\n",
      "Episode 170, Total Reward: 93\n",
      "Episode 180, Total Reward: 93\n",
      "Episode 190, Total Reward: 93\n",
      "Episode 200, Total Reward: 93\n",
      "Episode 210, Total Reward: 93\n",
      "Episode 220, Total Reward: 91\n",
      "Episode 230, Total Reward: 93\n",
      "Episode 240, Total Reward: 93\n",
      "Episode 250, Total Reward: 91\n",
      "Episode 260, Total Reward: 93\n",
      "Episode 270, Total Reward: 93\n",
      "Episode 280, Total Reward: 91\n",
      "Episode 290, Total Reward: 93\n",
      "Episode 300, Total Reward: 92\n",
      "Episode 310, Total Reward: 93\n",
      "Episode 320, Total Reward: 93\n",
      "Episode 330, Total Reward: 91\n",
      "Episode 340, Total Reward: 93\n",
      "Episode 350, Total Reward: 93\n",
      "Episode 360, Total Reward: 90\n",
      "Episode 370, Total Reward: 93\n",
      "Episode 380, Total Reward: 92\n",
      "Episode 390, Total Reward: 93\n",
      "Episode 400, Total Reward: 93\n",
      "Episode 410, Total Reward: 91\n",
      "Episode 420, Total Reward: 91\n",
      "Episode 430, Total Reward: 92\n",
      "Episode 440, Total Reward: 93\n",
      "Episode 450, Total Reward: 93\n",
      "Episode 460, Total Reward: 93\n",
      "Episode 470, Total Reward: 91\n",
      "Episode 480, Total Reward: 90\n",
      "Episode 490, Total Reward: 93\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5. -1.  0.  0.  0.]\n",
      " [ 0. -1. -1.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0. -1.  0.  0. 10.]]\n",
      "[[ 0. -1.  0.  0.  0.]\n",
      " [ 5. -1. -1.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0. -1.  0.  0. 10.]]\n",
      "[[ 0. -1.  0.  0.  0.]\n",
      " [ 0. -1. -1.  0.  0.]\n",
      " [ 5.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0. -1.  0.  0. 10.]]\n",
      "[[ 0. -1.  0.  0.  0.]\n",
      " [ 0. -1. -1.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 5.  0.  0. -1.  0.]\n",
      " [ 0. -1.  0.  0. 10.]]\n",
      "[[ 0. -1.  0.  0.  0.]\n",
      " [ 0. -1. -1.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  5.  0. -1.  0.]\n",
      " [ 0. -1.  0.  0. 10.]]\n",
      "[[ 0. -1.  0.  0.  0.]\n",
      " [ 0. -1. -1.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  5. -1.  0.]\n",
      " [ 0. -1.  0.  0. 10.]]\n",
      "[[ 0. -1.  0.  0.  0.]\n",
      " [ 0. -1. -1.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0. -1.  5.  0. 10.]]\n",
      "[[ 0. -1.  0.  0.  0.]\n",
      " [ 0. -1. -1.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0. -1.  0.  5. 10.]]\n",
      "[[ 0. -1.  0.  0.  0.]\n",
      " [ 0. -1. -1.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.]\n",
      " [ 0. -1.  0.  0.  5.]]\n"
     ]
    }
   ],
   "source": [
    "Helper.run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitocluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
