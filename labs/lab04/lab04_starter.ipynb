{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2fda1b0",
   "metadata": {},
   "source": [
    "# Lab 04 â€“ Dynamic Programming for Policy Evaluation Starter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f53813",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Introduce iterative policy evaluation and improvement for tabular MDPs using dynamic programming. Students reuse the Lab 03 environment to compute value functions numerically and visualize convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7064ac",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Implement policy evaluation via iterative Bellman updates.\n",
    "- Apply policy improvement to derive a better policy from value estimates.\n",
    "- Visualize convergence diagnostics and discuss stopping criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45850839",
   "metadata": {},
   "source": [
    "## Pre-Lab Review\n",
    "- Review the dynamic programming sections in [`old content/Section_3_MC.pdf`](../../old%20content/Section_3_MC.pdf), focusing on Bellman expectation and optimality equations.\n",
    "- Skim iterative update examples in [`old content/RL Solved Example - Updated.pdf`](../../old%20content/RL%20Solved%20Example%20-%20Updated.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4039eb",
   "metadata": {},
   "source": [
    "## In-Lab Exercises\n",
    "1. Port your Lab 03 MDP into a notebook or script that supports vectorized value updates.\n",
    "2. Implement iterative policy evaluation until the max value change drops below a tolerance.\n",
    "3. Integrate a policy improvement loop to derive a better policy.\n",
    "4. Plot the evolution of value estimates or residuals across iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d205f",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- Notebook or script containing policy evaluation/improvement implementations.\n",
    "- Short commentary on convergence speed and the impact of tolerance settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098e5e5",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [`old content/optimal.png`](../../old%20content/optimal.png) to discuss optimal policies visually.\n",
    "- Instructor-supplied starter code for tabular value iteration (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda4edd",
   "metadata": {},
   "source": [
    "### Policy Evaluation & Improvement\n",
    "This starter mirrors the tabular dynamic programming walkthrough referenced in the archived Monte Carlo/Dynamic Programming decks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ab304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "states = [(r, c) for r in range(4) for c in range(4)]\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "terminal_states = {(3, 3)}\n",
    "\n",
    "transition_reward = {}\n",
    "for state in states:\n",
    "    for action in actions:\n",
    "        r, c = state\n",
    "        if action == 'up':\n",
    "            r = max(0, r - 1)\n",
    "        elif action == 'down':\n",
    "            r = min(3, r + 1)\n",
    "        elif action == 'left':\n",
    "            c = max(0, c - 1)\n",
    "        elif action == 'right':\n",
    "            c = min(3, c + 1)\n",
    "        next_state = (r, c)\n",
    "        reward = 0 if next_state == (3, 3) else -1\n",
    "        transition_reward[(state, action)] = (next_state, reward)\n",
    "\n",
    "policy = {state: 'right' if state[1] < 3 else 'down' for state in states}\n",
    "for t_state in terminal_states:\n",
    "    policy[t_state] = None\n",
    "\n",
    "def policy_evaluation(policy, gamma=0.9, theta=1e-4):\n",
    "    V = {state: 0.0 for state in states}\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in states:\n",
    "            if state in terminal_states:\n",
    "                continue\n",
    "            action = policy[state]\n",
    "            next_state, reward = transition_reward[(state, action)]\n",
    "            v_new = reward + gamma * V[next_state]\n",
    "            delta = max(delta, abs(v_new - V[state]))\n",
    "            V[state] = v_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def policy_improvement(V, gamma=0.9):\n",
    "    new_policy = {}\n",
    "    for state in states:\n",
    "        if state in terminal_states:\n",
    "            new_policy[state] = None\n",
    "            continue\n",
    "        q_values = {}\n",
    "        for action in actions:\n",
    "            next_state, reward = transition_reward[(state, action)]\n",
    "            q_values[action] = reward + gamma * V[next_state]\n",
    "        best_action = max(q_values, key=q_values.get)\n",
    "        new_policy[state] = best_action\n",
    "    return new_policy\n",
    "\n",
    "V = policy_evaluation(policy)\n",
    "updated_policy = policy_improvement(V)\n",
    "print(\"Value function after evaluation:\")\n",
    "for r in range(4):\n",
    "    print([round(V[(r, c)], 2) for c in range(4)])\n",
    "print(\"\\nImproved policy:\")\n",
    "for r in range(4):\n",
    "    row_actions = []\n",
    "    for c in range(4):\n",
    "        row_actions.append(updated_policy[(r, c)] or 'T')\n",
    "    print(row_actions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
