{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f408e6c9",
   "metadata": {},
   "source": [
    "# Lab 09 â€“ Policy Gradient & Actor-Critic Methods Starter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8016dc",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Introduce policy gradient techniques for problems where value-based methods struggle. Students will implement REINFORCE and add baselines or simple actor-critic variants to reduce variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9adb9ce",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Derive and implement the REINFORCE algorithm with softmax policies.\n",
    "- Incorporate baseline functions to reduce gradient variance.\n",
    "- Extend to an actor-critic architecture with shared parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46b193",
   "metadata": {},
   "source": [
    "## Pre-Lab Review\n",
    "- Study [`old content/RL_Section8_pdf.pdf`](../../old%20content/RL_Section8_pdf.pdf) for policy gradient theorem derivations.\n",
    "- Review any policy gradient code snippets archived in [`old content/ALL_WEEKS_V5 - Student.ipynb`](../../old%20content/ALL_WEEKS_V5%20-%20Student.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac7c93",
   "metadata": {},
   "source": [
    "## In-Lab Exercises\n",
    "1. Implement the REINFORCE algorithm for a discrete-action environment (e.g., CartPole with softmax policy).\n",
    "2. Add a learned baseline or value function to create a variance-reduced estimator.\n",
    "3. Explore an actor-critic setup with simultaneous policy and value updates.\n",
    "4. Compare learning curves with and without baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9ca5a",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- Notebook containing REINFORCE and actor-critic implementations, plus comparison plots.\n",
    "- Reflection on variance sources and strategies for stabilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82d5f1",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [`old content/optimal.png`](../../old%20content/optimal.png) for visual discussion on optimal policies.\n",
    "- Suggested reading: Sutton & Barto Chapter 13 or equivalent policy gradient tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff58b4c",
   "metadata": {},
   "source": [
    "### Policy Gradient (REINFORCE) Skeleton\n",
    "Connect this template with the policy-gradient reading in `old content/RL_Section8_pdf.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d511b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    raise ImportError(\"Install gymnasium to run REINFORCE.\")\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, act_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def reinforce(env_name='CartPole-v1', episodes=500, gamma=0.99):\n",
    "    env = gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.n\n",
    "    policy = PolicyNetwork(obs_dim, act_dim)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            action_probs = policy(state_tensor)\n",
    "            dist = torch.distributions.Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(rewards):\n",
    "            G = reward + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        loss = 0\n",
    "        for log_prob, Gt in zip(log_probs, returns):\n",
    "            loss -= log_prob * Gt\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (episode + 1) % 20 == 0:\n",
    "            print(f\"Episode {episode + 1}, total reward: {sum(rewards):.1f}\")\n",
    "\n",
    "    env.close()\n",
    "    return policy\n",
    "\n",
    "# trained_policy = reinforce(episodes=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
