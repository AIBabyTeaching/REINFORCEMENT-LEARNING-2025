{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0c2fc3",
   "metadata": {},
   "source": [
    "# Lab 05 – Monte Carlo Methods Starter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7a853",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Explore Monte Carlo prediction and control techniques for episodic tasks. Students will build or reuse a simulator (e.g., Blackjack) to estimate value functions from sampled returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26a3d2",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Implement first-visit and every-visit Monte Carlo estimators.\n",
    "- Compare exploring starts with ε-greedy behavior policies.\n",
    "- Track policy improvement over episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2073f8",
   "metadata": {},
   "source": [
    "## Pre-Lab Review\n",
    "- Review the Monte Carlo lecture decks [`old content/Section_10_MonteCarlo_Dynamic_1.pdf`](../../old%20content/Section_10_MonteCarlo_Dynamic_1.pdf) and [`old content/Section_10_MonteCarlo_Example_1.pdf`](../../old%20content/Section_10_MonteCarlo_Example_1.pdf).\n",
    "- Revisit relevant segments in the legacy notebook [`old content/ALL_WEEKS_V5 - Student.ipynb`](../../old%20content/ALL_WEEKS_V5%20-%20Student.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a044227",
   "metadata": {},
   "source": [
    "## In-Lab Exercises\n",
    "1. Implement Monte Carlo prediction for state values using first-visit returns.\n",
    "2. Extend to action-value estimation with exploring starts.\n",
    "3. Introduce ε-greedy action selection and observe policy evolution.\n",
    "4. Discuss sample efficiency compared to dynamic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f80cf",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- Notebook with Monte Carlo code, learning curves, and commentary.\n",
    "- Brief reflection on trade-offs between Monte Carlo and DP methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b45e5",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [`old content/UpdateRuleExample.png`](../../old%20content/UpdateRuleExample.png) to visualize incremental averaging.\n",
    "- OpenAI Gymnasium environments for quick experimentation (e.g., Blackjack-v1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c47e8",
   "metadata": {},
   "source": [
    "### Monte Carlo Control Starter\n",
    "Adapted from the Blackjack and Monte Carlo examples in `old content/Section_10_MonteCarlo_Example_1.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    gym = None\n",
    "    print(\"Install gymnasium to run the full Blackjack example.\")\n",
    "\n",
    "def create_env():\n",
    "    if gym is None:\n",
    "        raise RuntimeError(\"gymnasium is required for this lab. Install it via pip install gymnasium[all].\")\n",
    "    return gym.make('Blackjack-v1', sab=True)\n",
    "\n",
    "def generate_episode(policy, env):\n",
    "    episode = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "    return episode\n",
    "\n",
    "def monte_carlo_control(num_episodes=50000, epsilon=0.1):\n",
    "    env = create_env()\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    Q = defaultdict(lambda: [0.0, 0.0])\n",
    "\n",
    "    def policy(state):\n",
    "        if random.random() < epsilon:\n",
    "            return env.action_space.sample()\n",
    "        values = Q[state]\n",
    "        return int(values[1] > values[0])\n",
    "\n",
    "    for episode_idx in range(1, num_episodes + 1):\n",
    "        episode = generate_episode(policy, env)\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = reward + 0.9 * G\n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                returns_sum[(state, action)] += G\n",
    "                returns_count[(state, action)] += 1\n",
    "                Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "        if episode_idx % 5000 == 0:\n",
    "            print(f\"Episode {episode_idx}: exploring starts Monte Carlo running...\")\n",
    "    return Q\n",
    "\n",
    "# q_values = monte_carlo_control(num_episodes=10000)\n",
    "# list(q_values.items())[:5]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
