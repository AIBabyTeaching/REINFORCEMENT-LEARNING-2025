{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d1fcca",
   "metadata": {},
   "source": [
    "# Lab 08 – Function Approximation & Deep Q-Networks Starter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f7f66",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Move beyond tabular methods by implementing a lightweight Deep Q-Network (DQN) for environments where state spaces are large or continuous. Students will focus on network design, replay buffers, and training stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edca78",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Configure a neural network to approximate action-value functions.\n",
    "- Implement experience replay and target network updates.\n",
    "- Monitor training curves and diagnose divergence or instability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e05ee6",
   "metadata": {},
   "source": [
    "## Pre-Lab Review\n",
    "- Watch a short DQN explainer video (instructor curated) to preview architecture choices.\n",
    "- Review the comparative figure [`old content/DQN_vs_Q.png`](../../old%20content/DQN_vs_Q.png) to recall tabular vs. deep approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a94d3",
   "metadata": {},
   "source": [
    "## In-Lab Exercises\n",
    "1. Set up the CartPole-v1 environment (or similar) using Gymnasium.\n",
    "2. Implement a minimal DQN with replay buffer, target network, and ε-greedy exploration.\n",
    "3. Train the agent for several episodes while logging loss, rewards, and ε schedule.\n",
    "4. Experiment with hyperparameters (learning rate, batch size, target update frequency) and document findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928d61d",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- Notebook detailing the DQN implementation, training curves, and tuning experiments.\n",
    "- Reflection on stabilization tricks (replay, target networks) and remaining challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455ec6c",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [`old content/optimal.png`](../../old%20content/optimal.png) to connect deep RL performance with optimal policy intuition.\n",
    "- Links to PyTorch/TensorFlow quickstarts for neural network setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363b119",
   "metadata": {},
   "source": [
    "### Deep Q-Network Scaffold\n",
    "Start from this lightweight DQN loop—align it with the DQN discussion in `old content/DQN_vs_Q.png` and related slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3633d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import gymnasium as gym\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install torch and gymnasium to run the DQN example.\") from exc\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=50000):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(torch.tensor, zip(*batch))\n",
    "        return states.float(), actions.long(), rewards.float(), next_states.float(), dones.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def dqn_train(env_name='CartPole-v1', episodes=200):\n",
    "    env = gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.n\n",
    "\n",
    "    policy_net = DQN(obs_dim, act_dim)\n",
    "    target_net = DQN(obs_dim, act_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "    buffer = ReplayBuffer()\n",
    "    gamma = 0.99\n",
    "    batch_size = 64\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.995\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = policy_net(state)\n",
    "                    action = int(torch.argmax(q_values))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32)\n",
    "            buffer.push((state.numpy(), action, reward, next_state, float(done)))\n",
    "            state = next_state_tensor\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "                next_q_values = target_net(next_states).max(1)[0]\n",
    "                targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "                loss = nn.functional.mse_loss(q_values, targets.detach())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        if episode % 10 == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            print(f\"Episode {episode}, reward {total_reward:.1f}, epsilon {epsilon:.3f}\")\n",
    "\n",
    "    env.close()\n",
    "    return policy_net\n",
    "\n",
    "# trained_policy = dqn_train(episodes=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
