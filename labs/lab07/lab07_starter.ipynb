{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38163e84",
   "metadata": {},
   "source": [
    "# Lab 07 – Value-Based Control (SARSA & Q-Learning) Starter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ea612",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Dive into control with temporal-difference methods by implementing SARSA and Q-learning on discrete environments. Students will analyze on-policy vs. off-policy behavior and exploration strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669afca",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Implement SARSA and Q-learning algorithms with ε-greedy exploration.\n",
    "- Compare learning curves and policy quality under different exploration rates.\n",
    "- Discuss stability considerations for on-policy vs. off-policy methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cad48f",
   "metadata": {},
   "source": [
    "## Pre-Lab Review\n",
    "- Review SARSA vs. Q-learning visual comparisons in [`old content/DQN_vs_Q.png`](../../old%20content/DQN_vs_Q.png).\n",
    "- Revisit control sections within [`old content/ALL_WEEKS_V5 - Student.ipynb`](../../old%20content/ALL_WEEKS_V5%20-%20Student.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcdb003",
   "metadata": {},
   "source": [
    "## In-Lab Exercises\n",
    "1. Select a benchmark environment (e.g., CliffWalking-v0, Taxi-v3) and reset seeds for reproducibility.\n",
    "2. Implement SARSA with decaying ε-greedy exploration; log episodic returns.\n",
    "3. Implement Q-learning with the same environment for comparison.\n",
    "4. Analyze stability, convergence speed, and sensitivity to hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87c238",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- Consolidated notebook summarizing SARSA and Q-learning implementations.\n",
    "- Short memo contrasting exploration/exploitation trade-offs observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bb945",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [`old content/optimal.png`](../../old%20content/optimal.png) for discussing optimal policy structures.\n",
    "- Links to Gymnasium documentation for environment-specific APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8ef8d",
   "metadata": {},
   "source": [
    "### SARSA vs. Q-Learning\n",
    "Starter adapted from the GridWorld Q-learning agent in `old content/ALL_WEEKS_V5 - Student.ipynb`. Compare on-policy and off-policy updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class EpsilonGreedyAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((*env.grid_size, len(env.actions)))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.env.actions)\n",
    "        row, col = state\n",
    "        action_index = np.argmax(self.q_table[row, col])\n",
    "        return self.env.actions[action_index]\n",
    "\n",
    "    def update_sarsa(self, state, action, reward, next_state, next_action):\n",
    "        row, col = state\n",
    "        action_idx = self.env.actions.index(action)\n",
    "        next_row, next_col = next_state\n",
    "        next_idx = self.env.actions.index(next_action) if next_action is not None else 0\n",
    "        td_target = reward + self.gamma * self.q_table[next_row, next_col, next_idx]\n",
    "        td_error = td_target - self.q_table[row, col, action_idx]\n",
    "        self.q_table[row, col, action_idx] += self.alpha * td_error\n",
    "\n",
    "    def update_q_learning(self, state, action, reward, next_state):\n",
    "        row, col = state\n",
    "        action_idx = self.env.actions.index(action)\n",
    "        next_row, next_col = next_state\n",
    "        td_target = reward + self.gamma * np.max(self.q_table[next_row, next_col])\n",
    "        td_error = td_target - self.q_table[row, col, action_idx]\n",
    "        self.q_table[row, col, action_idx] += self.alpha * td_error\n",
    "\n",
    "def run_episode(env, agent, algorithm='q_learning'):\n",
    "    state = env.reset()\n",
    "    action = agent.choose_action(state)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        next_action = agent.choose_action(next_state) if not done else None\n",
    "        if algorithm == 'sarsa':\n",
    "            agent.update_sarsa(state, action, reward, next_state, next_action)\n",
    "        else:\n",
    "            agent.update_q_learning(state, action, reward, next_state)\n",
    "        state, action = next_state, next_action if next_action is not None else agent.choose_action(next_state)\n",
    "    return total_reward\n",
    "\n",
    "# env = GridWorld(obstacles={(1, 1)})\n",
    "# agent = EpsilonGreedyAgent(env)\n",
    "# for episode in range(100):\n",
    "#     reward = run_episode(env, agent, algorithm='q_learning')\n",
    "#     if (episode + 1) % 20 == 0:\n",
    "#         print(f\"Episode {episode + 1}, total reward: {reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
