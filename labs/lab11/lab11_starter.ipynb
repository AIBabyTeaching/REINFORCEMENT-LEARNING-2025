{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71bd364",
   "metadata": {},
   "source": [
    "# Lab 11 â€“ Learning-Driven NAO Behaviors Starter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf38bd",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Bridge reinforcement learning policies with NAO robot behaviors. Students will deploy small state-space policies to the simulator (and physical robot if available), iterating on control loops informed by RL methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b54e7",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Adapt previously trained policies for NAO-friendly state and action spaces.\n",
    "- Integrate sensor feedback loops into NAO scripts.\n",
    "- Evaluate performance in simulation vs. hardware, documenting discrepancies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9afc7b",
   "metadata": {},
   "source": [
    "## Pre-Lab Review\n",
    "- Revisit [`old content/NAO_LAB_REBIRTH.pdf`](../../old%20content/NAO_LAB_REBIRTH.pdf) sections on behavior design and safety.\n",
    "- Review relevant control examples from [`old content/ALL_WEEKS_V5 - Student.ipynb`](../../old%20content/ALL_WEEKS_V5%20-%20Student.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69621fcb",
   "metadata": {},
   "source": [
    "## In-Lab Exercises\n",
    "1. Define a manageable NAO task (e.g., line following, posture stabilization, simple gesture sequencing).\n",
    "2. Map RL state features to NAO sensor readings; discretize or normalize as required.\n",
    "3. Deploy the policy to the simulator, collect logs, and adjust reward structures if needed.\n",
    "4. Test on a real NAO robot when available, noting latency, safety constraints, and differences from simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70d4c7",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- Code package containing policy implementation, NAO interface scripts, and configuration files.\n",
    "- Lab report summarizing deployment process, observations, and open issues for hardware trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1aa4f",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [`old content/howto.txt`](../../old%20content/howto.txt) for troubleshooting connection and environment issues.\n",
    "- Instructor guidance on NAO safety procedures and lab access protocols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125dd44",
   "metadata": {},
   "source": [
    "### NAOqi Setup Checklist\n",
    "The following steps are summarised from `old content/howto.txt`. Complete them before running the interaction cells.\n",
    "\n",
    "1. Install **Python 2.7 (32-bit)** and ensure `pip` is available.\n",
    "2. Create a dedicated virtual environment for NAO development.\n",
    "3. Download the NAOqi SDK (e.g., `pynaoqi-python2.7-2.5.5.5-win32-vs2013.zip`).\n",
    "4. Update your activation scripts (`activate.bat`/`activate.ps1`) to append the SDK's `lib` directory to `PYTHONPATH`.\n",
    "5. Activate the environment and verify that `from naoqi import ALProxy` succeeds.\n",
    "6. For simulator work, launch Choregraphe or the NAOqi virtual robot before attempting to connect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e6d25",
   "metadata": {},
   "source": [
    "### Policy Deployment Notes\n",
    "Use this cell to log configuration details from your simulator or physical NAO sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e311df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_log = {\n",
    "    \"session\": \"\",\n",
    "    \"policy_source\": \"Describe the RL policy or heuristic you are deploying.\",\n",
    "    \"environment\": \"Simulator or real robot.\",\n",
    "    \"observations\": [],\n",
    "    \"issues\": [],\n",
    "}\n",
    "\n",
    "deployment_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafee499",
   "metadata": {},
   "source": [
    "### Closed-Loop Control Placeholder\n",
    "Integrate your learned policy with NAO motion primitives. The helper below expects a policy function returning joint targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "try:\n",
    "    from naoqi import ALProxy\n",
    "except ImportError:\n",
    "    print(\"Ensure the NAOqi SDK is installed before running control loops.\")\n",
    "else:\n",
    "    ROBOT_IP = \"127.0.0.1\"\n",
    "    ROBOT_PORT = 9559\n",
    "\n",
    "    motion = ALProxy(\"ALMotion\", ROBOT_IP, ROBOT_PORT)\n",
    "\n",
    "    def run_policy_step(policy_fn, sensor_snapshot):\n",
    "        joint_targets = policy_fn(sensor_snapshot)\n",
    "        for joint_name, target in joint_targets.items():\n",
    "            motion.setAngles(joint_name, target, 0.2)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    def placeholder_policy(sensor_data):\n",
    "        return {\"HeadYaw\": 0.0, \"HeadPitch\": 0.0}\n",
    "\n",
    "    # sensor_data = motion.getAngles(\"Head\", True)\n",
    "    # run_policy_step(placeholder_policy, sensor_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
