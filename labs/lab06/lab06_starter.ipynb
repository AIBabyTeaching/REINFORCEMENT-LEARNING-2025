{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6219eaf5",
   "metadata": {},
   "source": [
    "# Lab 06 – Temporal-Difference Learning Starter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6ff11",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Transition from Monte Carlo methods to temporal-difference (TD) learning with incremental updates. Students will implement TD(0) and eligibility traces, comparing performance with Monte Carlo baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb2768",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Implement TD(0) prediction for value estimation.\n",
    "- Understand eligibility traces and TD(λ) intuition.\n",
    "- Compare convergence speed and sample efficiency versus Monte Carlo approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4b84e",
   "metadata": {},
   "source": [
    "## Pre-Lab Review\n",
    "- Revisit the solved examples highlighting TD behavior in [`old content/RL Solved Example - Updated.pdf`](../../old%20content/RL%20Solved%20Example%20-%20Updated.pdf).\n",
    "- Review any TD-focused notes embedded in [`old content/ALL_WEEKS_V5 - Student.ipynb`](../../old%20content/ALL_WEEKS_V5%20-%20Student.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6e2b5",
   "metadata": {},
   "source": [
    "## In-Lab Exercises\n",
    "1. Implement TD(0) for the random walk or Blackjack tasks from prior labs.\n",
    "2. Add eligibility traces to experiment with TD(λ) variants.\n",
    "3. Plot learning curves comparing TD and Monte Carlo approaches.\n",
    "4. Discuss the bias-variance trade-off inherent in TD learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aecdce",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- Notebook showcasing TD implementations, experiments, and plots.\n",
    "- Short write-up summarizing insights on TD vs. Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18cb23",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [`old content/DQN_vs_Q.png`](../../old%20content/DQN_vs_Q.png) to preview discussions about value approximation to come.\n",
    "- Open-source RL textbooks or Sutton & Barto Chapter 6 for supplemental reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99988844",
   "metadata": {},
   "source": [
    "### Temporal-Difference Starter\n",
    "Use this scaffold—mirroring the random walk TD examples from the archived notebook—to compare TD(0) and Monte Carlo estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class RandomWalk:\n",
    "    def __init__(self, n_states=5):\n",
    "        self.n_states = n_states\n",
    "        self.start_state = n_states // 2\n",
    "        self.terminal_left = -1\n",
    "        self.terminal_right = n_states\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self):\n",
    "        move = np.random.choice([-1, 1])\n",
    "        self.state += move\n",
    "        if self.state == self.terminal_right:\n",
    "            return self.state, 1, True\n",
    "        if self.state == self.terminal_left:\n",
    "            return self.state, 0, True\n",
    "        return self.state, 0, False\n",
    "\n",
    "\n",
    "def td_zero(env, V, alpha=0.1, gamma=1.0):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        next_state, reward, done = env.step()\n",
    "        if next_state in (env.terminal_left, env.terminal_right):\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + gamma * V[next_state]\n",
    "        V[state] += alpha * (target - V[state])\n",
    "        state = next_state\n",
    "\n",
    "def run_td_experiment(episodes=100):\n",
    "    env = RandomWalk()\n",
    "    V = defaultdict(float)\n",
    "    for episode in range(episodes):\n",
    "        td_zero(env, V, alpha=0.1)\n",
    "        if (episode + 1) % 20 == 0:\n",
    "            print(f\"Episode {episode + 1}: value estimates {dict(sorted(V.items()))}\")\n",
    "    return V\n",
    "\n",
    "# td_values = run_td_experiment(episodes=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
