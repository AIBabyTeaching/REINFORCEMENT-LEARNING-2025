# Lab 03 â€“ Markov Decision Processes

## Overview
Formalize sequential decision problems as Markov decision processes (MDPs). Students will define states, actions, rewards, and transitions, then validate their models with simple simulations.

## Learning Objectives
- Translate real-world scenarios into MDP components.
- Compute expected returns analytically for small state spaces.
- Simulate trajectories to confirm MDP reasoning.

## Prerequisites
- Comfort manipulating Python dictionaries or data classes.
- Background knowledge from Lab 02 on probability and stochastic processes.

## Pre-Lab Preparation
- Study Section 1 of [`old content/RL_1.pdf`](../../old%20content/RL_1.pdf) to revisit MDP notation and Bellman equations.
- Review the worked problems in [`old content/RL Solved Example - Updated.pdf`](../../old%20content/RL%20Solved%20Example%20-%20Updated.pdf).

## In-Lab Activities
1. Brainstorm a gridworld or robot navigation problem and enumerate its state/action spaces.
2. Encode transition probabilities and rewards in Python structures.
3. Calculate state-value functions for a fixed policy analytically.
4. Simulate sample trajectories to verify expectations against empirical returns.

## Post-Lab Deliverables
- MDP specification sheet (template provided by instructors).
- Python script or notebook that simulates a handful of trajectories and prints empirical returns.

## Resources
- [`old content/Mindmap.jpg`](../../old%20content/Mindmap.jpg) for a high-level concept map linking MDP components.
- Any supplementary notes shared in class for MDP modeling patterns.
